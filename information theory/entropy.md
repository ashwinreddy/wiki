entropy is a measure of uncertainty in a [[random variable]] by its [[expected value]] of information.

$$
H[X] = \mathbb{E}\left[h(x)\right]
$$

$$
H(X) = - \sum_{x \in \mathcal{X}} p(x) \log p(x)
$$

# properties

1. $H(X) \geqslant 0$