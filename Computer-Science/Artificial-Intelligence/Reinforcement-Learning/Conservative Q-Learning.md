---
bibliography: './Computer-Science/Artificial-Intelligence/Reinforcement-Learning/papers.bib'
---

**Conservative Q-Learning** [@kumar2020conservative] is an [[offline reinforcement learning]] algorithm that modifies [[Q-learning]] so not to be erroneously high on states it has not seen before. The algorithm comes up with a lower bound on the Q value, thereby making it conservative. We have access to a dataset $\mathcal{D}$ generated by policy $\pi_\beta(a \mid s)$.

$$
\hat{Q}^\pi_\text{CQL} = \arg\min_Q \alpha\left( \mathbb{E}\_{s \sim \mathcal{D}, a \sim \mu(a \mid s)}\left[Q(s,a)\right] - \mathbb{E}\_{s \sim \mathcal{D}, a \sim \hat{\pi}\_\beta(a \mid s)}\left[Q(s,a)\right] \right) + \underbrace{\frac{1}{2}\mathbb{E}\_{s,a,s' \sim \mathcal{D}}\left[ \left(Q - \hat{B}^\pi Q\right)^2 \right]}_{\text{standard Bellman error}}
$$

It says the Q-functions for OOD actions are pushed down. 

# Algorithm

1. Learn $\hat{Q}^\pi_{\text{CQL}}$ using offline data $\mathcal{D}$
2. Optimize policy w.r.t. $\hat{Q}^\pi_{\text{CQL}}$: $$\pi \leftarrow \arg\max_\pi \mathbb{E}_\pi\left[\hat{Q}^\pi_{\text{CQL}}\right]$$
3. Repeat

---