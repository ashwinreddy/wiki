The entropy of a random variable is the number of bits needed to describe it.

$$
H(X) = \mathbb{E}_{x \sim X}[I_X(x)]
$$

$$
H(X) = -\sum_i p_i \lg p_i
$$

[paper](https://link.springer.com/content/pdf/10.1007/s12064-020-00313-7.pdf)