The **surprisal** of an outcome $x$ is defined as the [[logarithm]] of the reciprocal of the probability of the outcome. 

$$
I(x) = \log_b \frac{1}{p(x)}
$$

The base $b$ of the logarithm is arbitrary, but common choices are 2 and $e$. The units of surprisal are then bits or nats, respectively.

Why is this a good definition?

1. The information of an event with 100% probability is 0.
2. The less probable, the more surprising (negative log is monotonic decreasing).