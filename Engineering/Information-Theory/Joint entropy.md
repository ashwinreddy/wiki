The **joint entropy** of a set of random variables is the [[entropy]] over all the possible outcomes in a [[joint probability distribution]]. 

\begin{equation}
H(X, Y) = H(X) + H(Y \mid X)
\end{equation}


The entropy of independent variables add in a straightforward manner.

\begin{equation}
H(X, Y) = H(X) + H(Y) \iff P(x,y)=P(x)P(y)
\end{equation}