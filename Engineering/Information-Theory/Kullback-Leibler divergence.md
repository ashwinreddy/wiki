The **Kullback-Leibler (KL) divergence** is a measure of inefficiency in using a distribution $Q$ to model distribution $P$.

$$
D_{KL} (P \parallel Q) = H(P, Q) - H(P)
$$