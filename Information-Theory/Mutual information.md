The **mutual information** between [[random variable]]s $X$ and $Y$ is given by [[entropy]] and [[conditional entropy]]

$$
I[X; Y] = H[X] - H[X \mid Y]
$$

It is related to [[Kullbackâ€“Leibler divergence]].

$$
I[X; Y] = D_{KL}\left(p(X, Y) \parallel p(X)p(Y)\right)
$$

## Properties

* $I[X;Y] = I[Y; X]$