The **Metropolis-Hastings algorithm** produces samples from a [[probability distribution]] $\pi$ that you can't directly sample from. The idea is to simulate a [[Markov chain]] whose [[stationary distribution]] is designed to be $\pi$. Then, in the long run, sampling from the chain is sampling from $\pi$.

We need a proposal distribution $Q(x, y)$ which proposes a new state $y$ when we're in the current state $x$.